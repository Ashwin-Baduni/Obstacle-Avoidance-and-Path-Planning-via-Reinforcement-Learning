# Reinforcement Learning Algorithms Suite

A comprehensive implementation and comparison of three fundamental reinforcement learning algorithms: Q-Learning, Deep Q-Network (DQN), and Actor-Critic, tested on a GridWorld environment.

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.10+-orange.svg)](https://tensorflow.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

## ğŸ¯ Project Overview

This project provides a clean, modular implementation of three core reinforcement learning algorithms, allowing for easy comparison and analysis of their performance on a standardized GridWorld environment. Each algorithm is implemented with modern Python practices, comprehensive logging, and detailed visualization capabilities.

### Implemented Algorithms

| Algorithm | Type | Key Features |
|-----------|------|--------------|
| **Q-Learning** | Tabular | Classic temporal difference learning, Q-table visualization |
| **Deep Q-Network (DQN)** | Deep RL | Neural network approximation, experience replay, Îµ-greedy exploration |
| **Actor-Critic** | Policy Gradient | Separate actor and critic networks, advantage-based learning |

## ğŸ—ï¸ Project Architecture

```
â”œâ”€â”€ main.py                    # Interactive main execution file
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .gitignore                # Git ignore rules
â”œâ”€â”€ algorithms/               # RL algorithm implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ q_learning.py         # Q-Learning implementation
â”‚   â”œâ”€â”€ dqn_agent.py          # Deep Q-Network implementation
â”‚   â””â”€â”€ actor_critic.py       # Actor-Critic implementation
â”œâ”€â”€ core/                     # Core system components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ environment.py        # GridWorld environment
â”‚   â””â”€â”€ utils.py             # Visualization and analysis utilities
â””â”€â”€ outputs/                  # Generated results (created at runtime)
    â”œâ”€â”€ q_learning/           # Q-Learning specific outputs
    â”œâ”€â”€ dqn/                  # DQN specific outputs
    â”œâ”€â”€ actor_critic/         # Actor-Critic specific outputs
    â””â”€â”€ comparisons/          # Cross-algorithm comparisons
```

## ğŸ® GridWorld Environment

The testing environment is a **3Ã—3 GridWorld** featuring:

- **Start Position**: (0,0) - Top-left corner
- **Goal Position**: (2,2) - Bottom-right corner  
- **Obstacle**: (1,1) - Center position (impassable)
- **Action Space**: 4 discrete actions (Up, Right, Down, Left)
- **Reward Structure**:
  - Goal reached: +100
  - Obstacle hit: -100
  - Each step: -1 (encourages efficiency)

```
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚  S  â”‚     â”‚     â”‚  S = Start
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚  â–   â”‚     â”‚  â–  = Obstacle
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚  G  â”‚  G = Goal
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11 or higher
- pip package manager
- (Optional) CUDA-compatible GPU for accelerated training

### Installation

1. **Clone the repository**:
   ```
   git clone https://github.com/Ashwin-Baduni/Reinforcement-Learning-Algorithms.git
   cd Reinforcement-Learning-Algorithms
   ```

2. **Install dependencies**:
   ```
   pip install -r requirements.txt
   ```

3. **Run the interactive suite**:
   ```
   python main.py
   ```

### Usage Options

The main interface provides several execution modes:

```
1. Q-Learning          - Run tabular Q-Learning algorithm
2. Deep Q-Network      - Run DQN with neural network approximation  
3. Actor-Critic        - Run Actor-Critic policy gradient method
4. Run All Algorithms  - Execute all three with comprehensive comparison
5. Exit               - Terminate the program
```

## ğŸ“Š Features & Capabilities

### Training Features
- **Real-time Progress Tracking**: Live progress bars with performance metrics
- **Configurable Hyperparameters**: Easy modification of learning rates, exploration, etc.
- **GPU Acceleration**: Automatic CUDA utilization for TensorFlow operations
- **Organized Output Management**: Structured file organization for all results

### Analysis & Visualization
- **Performance Plots**: Training curves with raw scores and moving averages
- **Algorithm Comparison**: Side-by-side performance analysis
- **Detailed Reports**: Comprehensive statistical analysis of results
- **Data Export**: CSV files for external analysis tools

### Output Organization
All results are automatically organized into timestamped files:

```
outputs/
â”œâ”€â”€ q_learning/
â”‚   â”œâ”€â”€ logs/           # Detailed training logs
â”‚   â”œâ”€â”€ visualizations/ # Performance plots  
â”‚   â””â”€â”€ data/          # Raw score data (CSV)
â”œâ”€â”€ dqn/               # DQN-specific results
â”œâ”€â”€ actor_critic/      # Actor-Critic results
â””â”€â”€ comparisons/       # Cross-algorithm analysis
    â”œâ”€â”€ visualizations/ # Comparison plots
    â””â”€â”€ reports/       # Performance summaries
```

## ğŸ”§ Technical Implementation

### Q-Learning
- **Algorithm**: Temporal Difference Learning with Q-table
- **Exploration**: Îµ-greedy strategy
- **Episodes**: 1,000 training episodes
- **Features**: Complete Q-table visualization, optimal policy display

### Deep Q-Network (DQN)
- **Network Architecture**: 2 hidden layers (24 neurons each)
- **Experience Replay**: 2,000-step memory buffer
- **Exploration**: Decaying Îµ-greedy (1.0 â†’ 0.01)
- **Episodes**: 500 training episodes
- **Input Encoding**: One-hot state representation

### Actor-Critic
- **Architecture**: Separate actor and critic networks
- **Actor**: Softmax policy output (action probabilities)
- **Critic**: Value function approximation
- **Learning**: Advantage-based policy gradients
- **Episodes**: 500 training episodes

## ğŸ“ˆ Performance Metrics

The suite tracks and reports multiple performance indicators:

- **Episode Scores**: Cumulative reward per episode
- **Learning Speed**: Episodes required to reach 90% performance
- **Stability**: Performance consistency (standard deviation)
- **Final Performance**: Average of last 100 episodes
- **Convergence Analysis**: Moving average trends

## ğŸ› ï¸ Customization

### Environment Modification
Modify `core/environment.py` to:
- Change grid size
- Adjust reward structure  
- Add multiple goals or obstacles
- Implement different action spaces

### Algorithm Tuning
Each algorithm file contains configurable hyperparameters:
- Learning rates
- Exploration parameters
- Network architectures
- Training episodes

### Visualization Enhancement
Extend `core/utils.py` to add:
- Custom plot styles
- Additional metrics
- Export formats
- Interactive visualizations

## ğŸ¤ Contributing

Contributions are welcome! Areas for enhancement:

- Additional RL algorithms (PPO, A3C, etc.)
- More complex environments
- Hyperparameter optimization
- Performance benchmarking
- Documentation improvements

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built with modern Python practices and clean architecture principles[1]
- Implements state-of-the-art reinforcement learning techniques[2]
- Uses professional package management and dependency organization[3]

## ğŸ“ Contact

**Ashwin Baduni**
- GitHub: [@Ashwin-Baduni](https://github.com/Ashwin-Baduni)
- Email: baduniashwin@gmail.com

---
